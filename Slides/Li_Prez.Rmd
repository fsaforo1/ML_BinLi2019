---
title: "Predicting Hit Songs - Hot 100 Billboard Chart"
author:
- Frank K. Saforo
- Chris J. Ezelle
date: February 22, 2019
output:
  slidy_presentation:
    #incremental: true
    footer: "F. K. Saforo, C. J. Ezelle, 2/25/2019"
---  

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>      

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(easypackages)
libraries('data.table', 'testthat', 'gridExtra', 'corrplot', 'GGally', 'reshape','caret',
          'MASS', 'DT', 'psych', 'ggplot2', 'e1071', 'pdp', 'xgboost', 'htmlwidgets', 'pryr',
          'dplyr', 'Amelia', 'mice', 'tidyverse', 'RColorBrewer', 'rpart', 'caTools', 'treemap',
          'AppliedPredictiveModeling', 'rpart.plot', 'randomForest', 'pROC', 'xgboost',
          'Matrix', 'vcd', 'RANN', 'plyr', 'plotly', 'kableExtra', 'gbm', 'PRROC', 'Ckmeans.1d.dp')
```

## Overview


- Introduction  


- The Data and EDA  


- Model Building and Evaluation    


- Results and Interpretation    


- Recommendation and Conclusion



## Introduction

- Data was extracted from the Spotify API 
- Get out of bed



## Collecting the Data
<center>
```{r, out.width = "60%", echo=FALSE}
knitr::include_graphics("./Slides/prz_000.png")
```
</center>   

 - **Latest data retrieval was on December 28, 2018**


## Data Snap  
```{r, echo=FALSE, warning=FALSE, message=FALSE}
datatable(spotTest[1:10,], class = 'cell-border stripe')
```

```{r, warning=FALSE, message=FALSE}
str(newdata)
object_size(newdata)
```




## The Data...   

  - Tempo: Beats Per Minute (BPM) of the song.    
  - Energy: The energy of a song, the higher the value, the more energetic.     
  - Danceability: The higher the value, the easier it is to dance to this song.     
  - Loudness: The higher the value, the louder the song (in dB).    
  - Valence: The higher the value, the more positive mood for the song.     
  - Length: The duration of the song.    
  - Acousticness: The higher the value the more acoustic the song is.      
  - Release Year: The year each song was released.     
  - **Popularity (Target Variable)**: The higher the value (on a scale of 0 to 100) the more popular the song is.

**Source:** See features description [here](http://static.echonest.com/SortYourMusic/)


## Most Popular Songs of 2018

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
top5plot
```


## Top 100 Songs by Key and Emotion   

<div class="col2">  
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "80%"}
treemap(ctone2, index="keys", vSize="count", type="index", 
        palette="Pastel2", title="Top 100 Songs Key charactersics", fontsize.title=12)

treemap(tone2, index="keylabel", vSize="count", type="index", 
        palette="Pastel2", title="Top 100 Songs Key charactersics and Emotion", fontsize.title=11)
```
</div>

## Are danceable songs popular?
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("./Slides/prz_dance.png")
```


## How does mood affect the popularity of a song?
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("./Slides/prz_mood.png")
```


## Energy Vs. Popularity
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("./Slides/prz_energy.png")
```

## Model Building and Evaluation

<center>
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("./Slides/prz_mod.png")
```
</center>

## The Regression Problem 

Popularity is a copntinuous variable that ranges between 0 and 1 
```{r, echo=FALSE}
print(paste("Minimum Popularity is", 0.1))
print(paste("Maximum Popularity is", 0.9))
```

Models considered:    
    - Model appropriate for the random component of the response variable; a **GLM with beta random component**.      
    - A  model that relaxes assumptions of linearity and probability distribution; a nonparametric model **Multivariate Adaptive Regression Splines**
    
### The Beta Distribution
Ferrari and Cribari-Neto, 2004 proposed a reparametrization of the **Beta Distribution** , $\mu=p/(p+q)$ and $\phi=p+q$, with the density function being:
$$f(y \mid \mu,\phi)=\frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma((1-\mu)\phi)}y^{\mu\phi-1}(1-y)^{(1-\mu)\phi-1}, \;\;\; \;\;0<y<1$$

$$E(y)=\mu$$
and
$$Var(y)=\frac{\mu(1-\mu)}{1+\phi}$$
The parameter $\phi$ is known as the precision parameter. Notice that for fixed $\mu$, $\phi$ gets larger and variance of $y$ gets smaller.
The inverse of $\phi^{-1}$ is also known as the dispersion parameter.   

This parameterization is particularly useful in linear regression since it involves modeling the mean or the expected value of the response variable$(y)$. Thus $\mu$ is the mean of the response variable.  
  
## The Beta [GLM] Model  

Following the latter parameterization of the beta distribution proposed by Ferrari and Cribari-Neto(2004) above, let $y_1, y_2, ...,y_n$ be a random sample such that $y_i \sim Beta(\mu_i,\phi)\;\;\;\;i=1,2,...,n$. Then the beta regression model is defined as:   

$$g(\mu_i)=x_i^T\beta$$  
where $\beta=(\beta_1, \beta_2,...,\beta_p)$ is a $p\times1$ vector of unknown parameters$(p<n), \;\;x_i=(x_{i1},(x_{i},...,(x_{ip})^T$ is the vector of $p$ regressors or the independent variables of this model. $x_i^T\beta=\eta_i$ is the linear predictor or the systematic component of the model.   

Also note that the link function represented by $g(*):(0,1) \implies \mathbb{R}$ is strictly increasing in $*$ and twice differentiable hence continuous everywhere.   

Due the the link, random (beta distribution) and systematic component (linear predictor), the beta regression model can be modelled seen as a Generalized Linear Model (GLM) (Agresti et al. Categroical Data Analysis). This offers flexibility in the choice of the link function to suit particular cases.  Some common link functions are:


### Some Common Link functions

 - **logit** $g(\mu) = log(\frac{\mu}{1-\mu})$, thus $\mu=\frac{e^{x_i^T\beta}}{1+e^{x_i^T\beta}}$
 - **log-log** $g(\mu) = log(-log(\mu))$
 - **Probit** $g(\mu) = \phi^{-1}(\mu)$
 - **Complimentary log-log** $g(\mu) = log(-log(1-\mu))$
 - **Cauchit** $g(\mu) = tan(\pi (\mu - 0.5))$   
 - **Identity** $g(\mu) = \mu$

Like with most GLM, the inverse function $\mu=g^{-1}(x_i^T\beta)$, is a function (linear combination) of the regression parameters $\beta$, hence the maximum likelihood estimation is adopted.



## The Classification Problem
This problem is a classic case of class imbalance with the class of interest being roughly 10 percent of available data.  

```{r, echo=FALSE, warning=FALSE, message=FALSE}
prop.table(table(modData$target))
```

A rather simple criteria was adopted. Fit models that:  

  1. produces the best prediction of classes, and with a variable selection feature   
  
  2. produces the best prediction, and with the best variable explanation
  
Models that meet criteria 1: Models that employ Gradient Boosting Algorithms OR Bagging Algorithms.
  Due to serious computational constraints only the **XGBoost, C5.0** models were considered with optimal parameter turning.   
  
Models that meet criteria 2:   Models that employ Stacking or Bagging algorithms. Here, only the **logistic regression and random forest models** were considered.


## Model Building - Beta Regression Model 

  - First conduct feature selection using the Recursive Feature Elimination (RFE) algorithm        
  
  - Cross-validate model accross a combination of its tuning parameters using selected variables from the RFE: **the link function**, **parameter estimation criteria**, and the **precision parameter**


### Recursive Feature Elimination 

```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
Target_Profile
kable(varImp(Target_Profile))%>% kable_styling(bootstrap_options = c("bordered", "hover","striped"),
                  font_size = 15,
                  full_width=FALSE)   # relative variables of importance
```

## The Beta Model 
**Tuning Parameters of Beta Model**
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
betareg
```

### Probing the beta Model


## MARS Model  

  - a piecewise linear model that captures the nonlinearity aspect of polynomial regression by assessing cutpoints (knots) similar to step functions.    
  - Cross-validate to tune model complexity by pruning for an optimal combination of model hyperparameters    
  - **Pruning Parameters**: **degree of interactions** and the **number of retained terms**

### 10-Fold Cross Validation 

```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
# best model
tuned_mars$bestTune
##    nprune degree
## 14     34      2

# plot results
ggplot(tuned_mars)
```

## MARS Model Analysis

```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
# Coefficients of the model
coef(tuned_mars$finalModel) %>%
  tidy() %>%
  dplyr::filter(stringr::str_detect(names, "\\*"))


# variable importance plots
VarIm
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "gcv") + ggtitle("GCV")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss") + ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, ncol = 2)

p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
p2 <- partial(tuned_mars, pred.var = "Year_Built", grid.resolution = 10) %>% autoplot()
p3 <- partial(tuned_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), grid.resolution = 10) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

### Test Set Diagnostics
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
```

